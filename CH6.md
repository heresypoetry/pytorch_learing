# CH6: 使用神经网络拟合数据：



## 人工神经网络：

神经网络是深度学习的核心。

- **神经元**： 神经元是神经网络的基本构件，其核心是输入的线性变换，然后应用于一个固定的**非线性函数**，**即激活函数**。 可以用公式表达为$o = f(\omega x + b)$. 其中，其中，$\omega，b$是权重和偏置，而f是激活函数。
- **多层网络**： 令$x_{i+1}= f(\omega_i x + b_i), i = 0,\dots,n$ .  $x_{i+1}$是神经元。（$\omega_0$是一个矩阵，$x,b$是向量，所以输出是向量）。 前一个神经元的输出被当作下一个神经元的输入。这些神经元就组成一个多层神经网络。
- 激活函数的作用：

1. 在模型的内部，激活函数允许输出函数在不同的值上有不同的斜率。这是线性函数无法做到的。通过设置不同的斜率来让神经网络近似不同的函数。

2. 可以将上一层的输出结果集中到需要的范围内。具体的方法有：使用torch.nn.Hardtanh的简单激活函数，可以把$[a,b]$区间以外的值，小于a的映射为a, 大于b的映射为b。 另一类运行良好的函数是torch.nn.Sigmoid(), $\frac{1}{1+e^{-x}}$, torch.tanh以及一些其他的函数，这些函数在x趋于负无穷大时组建区域0或-1，函数在x==0时具有基本恒定的斜率，神经元对它很敏感。

- 其它激活函数：ReLU（用于整流线性单元）目前被认为是性能最好的

![CH6-1](/Users/apple/Library/Application Support/typora-user-images/ml_learn_ch_16.png)

-  激活函数的特性：

1. 激活函数是非线性的。若没有激活函数，重复应用$\omega \cdot x +b$, 最后的结果一定是线性的。
2. 激活函数是可微的。因此可以计算它们的梯度。
3. 激活函数通常至少有一个**敏感范围**（或叫饱和范围）和许多**不敏感范围**（不饱和范围），在敏感范围内，对输入的变化会导致输出的产生相应的变化，而在不敏感范围内，输入的变化导致输出的变化很小或没有。

- 学习对神经网络的意义：

可以得到近似**高度非线性过程**的模型。例如下面的图片：

![CH6——2](/Users/apple/Library/Application Support/typora-user-images/ml_learn_CH6_2.png)

## PyTorch nn 模块：

torch.nn 是一个专门用于神经网络的子模块。

- nn.Linear:

PyTorch 提供的所有 nn.Module 的子类都定义了它们的__calL_0方法这允许我们实例化一个nn.Linear，并像调用函数一样调用它。

...

- 最终完成一个神经网络：

由于历史原因，第一个线性模型和激活层通常被叫做**隐藏层**。因为它的输出一般看不到而是传入下一层。

nn 提供了nn>Sequential 来连接几个模型。将前一个模块的输出作为后一个模块的输入。中间用激活函数来传递。

- 检查参数：

可以对model使用parameters方法或者named_parameters可以从每个线性模块中收集权重和偏置，以及模块的名字。

Sequential接受OrderdDict, 可以用这样的方法给Sequential中的每个模块命名。



（本章节未完成，日后修改）

